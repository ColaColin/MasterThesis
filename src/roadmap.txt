Target      What
14.2.2020   Implement distributed training server and worker, so be ready to train at full "scale"

21.2.2020   Run first verification that training at full scale works up to a 99%+ perfect play on the testset.

28.2.2020   Time buffer to fix bugs and verify things really work.
            [X] try out weak2, does it reach the 4 correct answers per position?! Yes, it reaches 4.02, just shy of the 4.07 oracle devs stated.
            [X] change format of position database to human readable text, include game result
            [X] update the dataset tester for that
            [X] write unit tests for the above
            [X] supervised training scripts & direct network test script.
            [X] generate dataset that only takes one of the positions reached  per game to provide more data to the win-prediction.
            [X] update command/remote eval: Track only one dataset: Raw network moves, Raw network wins, Fully iterated policy moves. Rerun evaluation for old runs again...
            [X] insight into played games: Make them visible, including an insight into the network output and mcts results on them.
            [X] the broken win accuracy test code requires new supervised runs. Do 2 train window sizes only.

                [X] 2 block 1M
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  87.41  |  74.64  
           800000 |  89.02  |  75.98   <<< best

                [X] 5 block 1M
Training set size | moves % | wins %
------------------+---------+---------
           400000 |  90.10  |  76.32
           800000 |  91.63  |  77.47   <<< best

                [X] 5 block 1Mv2
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  91.12  |  78.18  
           800000 |  92.44  |  79.23   <<< best

                [X] 10 block 1M
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  90.80  |  76.63  
           800000 |  92.37  |  77.87   <<< best

                [X] 10 block 1Mv2
Training set size | moves % | wins %
------------------+---------+---------
           400000 |  91.82  |  78.66
           800000 |  93.00  |  79.67   <<< best

                [X] 20 block 1M
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  91.35  |  77.08  
           800000 |  92.98  |  78.23   <<< best

                [X] 20 block 1Mv2
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  92.12  |  78.90  
           800000 |  93.49  |  79.93   <<< best

            [X] do a hyperparameter search with a fixed network, fixed lr, etc. Search only 4 parameters with a 2 block network and 400 expansions: drawValue, cpuct, fpu, alphaBase. 3h runs, 50 iterations.
               -> searched parameters, sorted by accuarcy
N, acc, alphaBase, cpuct, drawValue, fpu
43,81.74,12.5,0.9267,0.4815,0.9168
62,81.61,20.38,1.545,0.6913,0.8545
23,81.3,12.55,0.8988,0.3037,1.0
49,81.18,12.57,0.8172,0.5063,1.0
57,81.12,16.13,0.7032,0.4977,0.7638
58,81.03,16.06,0.744,0.2199,0.8136
33,80.92,16.19,0.6191,0.3296,0.6402
55,80.9,12.65,1.13,0.04908,0.7947
39,80.86,12.7,1.049,0.3215,0.934
26,80.84,12.75,1.27,0.165,1.0
19,80.79,12.49,0.5677,0.1157,0.8806
28,80.74,12.65,0.7292,0.2831,0.8584
36,80.72,12.42,0.7851,0.2473,0.8453
27,80.7,12.68,0.5422,0.4029,0.7281
20,80.64,12.71,0.8139,0.000533,0.7598
59,80.59,16.27,0.7108,0.3427,0.8777
48,80.53,12.58,0.7973,0.5279,0.7396
35,80.52,20.69,1.502,0.7027,0.907
25,80.51,25.09,1.19,0.01503,0.161
34,80.51,12.62,0.4484,0.2873,0.8593
37,80.46,12.61,0.8516,0.09574,1.0
24,80.44,12.24,1.004,0.3101,0.8037
44,80.43,12.55,0.9343,0.2638,0.6285
64,80.42,20.38,1.811,0.8025,0.7701
30,80.41,12.86,1.028,0.002271,0.9282
53,80.35,16.25,0.7826,0.4555,0.5557
51,80.32,12.87,1.225,0.2191,0.7537
10,80.22,12.84,0.3788,0.1524,0.9715
11,80.12,29.87,2.12,0.8169,0.04463
45,80.11,14.38,2.654,0.8249,0.358
12,80.08,20.0,1.589,0.8373,0.8544
22,80.05,7.881,1.813,0.8035,0.648
29,80.04,12.39,1.072,0.2192,0.968
14,79.96,12.73,0.5443,0.4315,0.696
46,79.88,12.49,1.085,0.5019,0.8301
2,79.8,6.962,0.7809,0.1863,0.3456
15,79.76,19.79,1.537,0.8838,0.7593
61,79.74,15.93,0.6977,0.3855,0.6747
17,79.65,12.78,0.5301,0.4677,0.6818
52,79.64,16.16,0.4541,0.09906,0.6
16,79.58,3.256,3.414,0.162,0.4778
8,79.48,19.83,1.534,0.9591,0.8047
21,79.38,12.53,0.4773,0.0,0.5039
7,79.13,14.95,2.627,0.5478,0.7442
60,79.12,7.294,3.001,0.5067,0.1211
47,79.03,29.74,3.953,0.08781,0.07062
31,79.02,12.41,0.468,0.586,0.8486
63,78.99,16.24,0.5017,0.552,0.7515
18,78.96,9.32,2.82,0.5501,0.9492
5,78.84,14.27,3.462,0.1404,0.1981
3,78.71,13.71,3.348,0.4192,0.6852
54,78.56,10.74,0.8472,0.8536,0.1412
32,78.18,17.56,2.504,0.9372,0.4217
38,78.18,12.71,3.165,0.7248,0.5209
1,78.04,14.26,4.392,0.000114,0.3023
9,77.87,3.573,4.895,0.9649,0.7021
65,77.63,21.34,5.016,0.3277,0.3797
13,77.62,11.77,3.906,0.4005,0.7516
50,76.32,13.26,5.128,0.4109,0.3728
56,76.23,28.36,5.838,0.2482,0.1385
4,76.2,8.52,5.299,0.02739,0.6705
40,75.14,28.13,5.497,0.7549,0.6893
41,74.92,13.62,5.681,0.7649,0.7862
42,73.94,12.45,5.672,0.3793,0.2894
6,69.59,5.502,0.25,0.3374,0.03511

     -> the top ones are all very similar, with few exception, pick the two best, the best one represents a typical "best", the 2nd has an unusual cpuct value of above 1.5



               -> suggested parameters by prev work: drawValue 0.5, cpuct = 4, fpu = 0 (per deep mind), alphaBase = 7 (which yields alpha of 1, since alphaBase is divided by the number of legal moves)
            [X] Build a selfplay speed measurement system and run tests to show how much variation there is in the measurements.
            [X] Decide on the network size to use based on supervised results, 
            [X] Restart secure play workers
            [X] "Iteration 170% completed" -> why would it say that? Why not stop at 100%?!
            [X] raise for status and timeout for StatesDownloader2!
            [-] v2 of the training code should allow to restart the process  Skip, don't really need that.
            

            [ ] do a baseline run. Those settings then also need to be what is "factor 1" of training speed. Whatever % that gets on 1Mv2 is the first training goal to reach faster...
               Goal to reach is 90.5% mcts accuracy. Measure cost to reach that iteration. Stop after 10 iterations with no further mcts accuracy improvement

               3 Sets of potential parameters were identified:
                    name      | alphaBase | cpuct  | drawValue | fpu
                    hyperopt1 | 12.5      | 0.9267 | 0.4815    | 0.9168
                    hyperopt2 | 20.38     | 1.545  | 0.6913    | 0.8545
                    prevWork  | 7         | 4      | 0.5       | 0

               [ ] hyperopt1
                    [X] run1: 90.01% in iteration 22. Failed to reach 90.5%
                    [X] run2: 90.01% in iteration 25. Failed to reach 90.5%
                    [X] run3: 90.03% in iteration 20. Peak at 90.26% in iteration 25. Failed to reach 90.5%
                    [X] run4: 90.06% in iteration 20. Peak at 90.46% in iteration 33. Failed to reach 90.5%
                    [X] run5: 90.11% in iteration 19. Failed to reach 90.5%


               [X] hyperopt2
                    [X] run1: 90.5% in iteration 19. Peak 91% in iteration 33.
                    [X] run2: 90.41% in iteration 19. Failed to reach 90.5% :(
                    [X] run3: 90.07% in iteration 17. 90.51 in iteration 27.
                    [X] run4: 90.09% in iteration 19. Peak 91% in iteration 38.
                    [X] run5: 90.2% in iteration 19. Peak 90.7% in iteration 30.

               [X] prevWork
                    [X] run1: peak at 89.88% in iteration 49. 
                    [X] run2: peak at 89.76% in iteration 46.
                    [X] run3: peak at 89.86% in iteration 55.
                    [X] run4: peak at 90.24% in iteration 53.
                    [X] run5: peak at 89.95% in iteration 44.

               [ ] run the cost evaluator

            [X] implement a backup script, which downloads all data from the server and restores it into my local database.

            - use the same optimizer as everyone else did:
                - SGD with momentum of 0.9
                - l2 weight regularition: 0.0001
                - LR: 0.2, 0.02, 0.002, 0.0002, drop every 20 epochs?
            - implement position deduplication in the stream trainer

            - Allow to play vs any network on the web?
            - optimization: Insta-play forced moves without "thinking" at all? Do not produce frames for them?
                - what happens with such games currently anyway? What gets passed into the gpu for evaluation if there are no more nodes to expand?!
            - do this later, perf is ok for now: have a look at apex to use fp16 where possible
            - maybe have a relative playing strength comparision of the iterations?

20.4.2020   Figure out the best way to measure learning efficiency, as per chapter 4.1 of the proposal.

1.5.2020    The simple parts of the writing are completed: introduction, describe the known improvements, describe the algorithm, etc.

08.5.2020   Have a look at possible metrics to determine if the training suffers from any form of forgetting-and-relearning, as per chapter 4.2 of the proposal

            Implement known improvements that were stated as obligatory in the proposal:
15.5.2020       X more filters in the network heads
15.5.2020       X predict a drawing probability directly
15.5.2020       X handle duplicate training positions
               [X] fix issue of "forgotten" frames by tracking which frames have been removed from the waitBuffer-cycle and readding them as new frames if they show up again
               [X] assert failed with a value of 0?!
               do a few test runs:
               [~] 0.2 weight: looks not much difference, though it failed on a bug in iteration 25.
               [X] 0.5 weight: peak 91.35% in iteration 37. 90% in 18, 91% in 28.
               [ ] 0.8 weight:

15.5.2020      [X] use a cyclic learning rate
               [X] run a test: looks good, seems to help over baseline
15.5.2020      [X] improve training window size handling
               [X] run a test: looks ok. No great improvement however.
                
15.5.2020      [X] use playout caps
               [X] run test: first test failed due to bad settings on the cheapMcts instance. It degraded game variety massively.
               -> more tests show either a "degrade game variety massively as it reaches 90.5% in record pace to then break off, or no improvements at all, depending on hyperparameter.
               -> it might be that 30 expansions is just not such a good idea?!
               -> or maybe playout cap randomization limits the final playing strength due to less higher quality games being played.
               [ ] implement a "NoOpPolicyIterator", which just forwards through the network once and returns the network output directly.

15.5.2020       - squeeze-and-excitation networks
15.5.2020       - predict the opponens move



22.5.2020    Do a new training run to 99%+ and measure improvements made by these changes.

            Implement suggested new improvements, first the network architecture changes:
22.5.2020       - gather-excity blocks
22.5.2020       - mobilenetv3 network blocks

1.6.2020   Do a training run to 99%+ and measure improvements of the network architecture changes.

01.6.2020   Writing about proposed ideas is completed.

            Implement more conceptual changes to the algorithm:
01.6.2020       - Play games as trees and evaluate with a training run
05.6.2020       - Use internal network features of previous iterations as automatic auxilary targets and evaluate it with a training run
19.6.2020       - Adapt the self-playing phase to search for hyperpameters and evaluate it with a training run

23.6.2020   Write about measured results

            Implement optional known improvements:
26.6.2020        - Combine two networks in the search process and evaluate with a training run
03.7.2020        - Use the kullback-leibler distance to determine how certain the tree search is on a position and evaluate with a training run
                 - more optional stuff if time allows...

15.7.2020   Finalize writing            

Some extra ideas:
     - exploration is really stupid in the lategame, try out to reduce temperature by ply 12 instead of 30
     - the fpu calculation based on "Am I winning?"
     - "if I am winning, do not explore at all"
     - play out forced moves automatically without any policy iteration on them. Do not record them as training frames.
          -> this also would need to happen inside the mcts tree?!
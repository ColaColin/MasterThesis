Target      What
14.2.2020   Implement distributed training server and worker, so be ready to train at full "scale"

21.2.2020   Run first verification that training at full scale works up to a 99%+ perfect play on the testset.

28.2.2020   Time buffer to fix bugs and verify things really work.
            [X] try out weak2, does it reach the 4 correct answers per position?! Yes, it reaches 4.02, just shy of the 4.07 oracle devs stated.
            [X] change format of position database to human readable text, include game result
            [X] update the dataset tester for that
            [X] write unit tests for the above
            [X] supervised training scripts & direct network test script.

// the moves peaked at 92% in most runs, wins stayed close to random. Maybe the better dataset can change that?
// 1M dataset, dedupe, p=0.5, filterTrivial, strong
Training set size | moves % | wins %           
------------------+---------+---------         
           100000 |  83.02  |  38.71           
           200000 |  87.16  |  37.68           
           300000 |  87.81  |  37.91           
           400000 |  90.20  |  35.84           
           500000 |  90.71  |  36.28           
           600000 |  87.02  |  40.38           
           700000 |  86.56  |  41.17           
           800000 |  90.10  |  37.75   <<< best
            [X] generate dataset that only takes one of the positions reached  per game to provide more data to the win-prediction.
            [X] supervised training on that new dataset
Training set size | moves % | wins %
------------------+---------+---------
           200000 |  88.88  |  38.01
           400000 |  86.95  |  43.00
           600000 |  91.13  |  39.41
           800000 |  90.95  |  40.15   <<< best
           
            [X] update command/remote eval: Track only one dataset: Raw network moves, Raw network wins, Fully iterated policy moves. Rerun evaluation for old runs again...
            [X] insight into played games: Make them visible, including an insight into the network output and mcts results on them.
            [ ] do a hyperparameter search with a fixed network, fixed lr, etc. Try to search only 3 parameters.
            [ ] do a baseline run. Those settings then also need to be what is "factor 1" of training speed. Whatever % that gets on 1Mv2 is the first training goal to reach faster...
            [ ] Build a selfplay speed measurement system and run tests to show how much variation there is in the measurements.

            - use the same optimizer as everyone else did:
                - SGD with momentum of 0.9
                - l2 weight regularition: 0.0001
                - LR: 0.2, 0.02, 0.002, 0.0002, drop every 20 epochs?
            - implement position deduplication in the stream trainer

            - Allow to play vs any network on the web?
            - optimization: Insta-play forced moves without "thinking" at all? Do not produce frames for them?
                - what happens with such games currently anyway? What gets passed into the gpu for evaluation if there are no more nodes to expand?!
            - do this later, perf is ok for now: have a look at apex to use fp16 where possible
            - maybe have a relative playing strength comparision of the iterations?

20.3.2020   Figure out the best way to measure learning efficiency, as per chapter 4.1 of the proposal.

03.4.2020   Have a look at possible metrics to determine if the training suffers from any form of forgetting-and-relearning, as per chapter 4.2 of the proposal

            Implement known improvements that were stated as obligatory in the proposal:
10.4.2020       - squeeze-and-excitation networks
10.4.2020       X more filters in the network heads
10.4.2020       - use a cyclic learning rate
17.4.2020       - use playout caps
17.4.2020       - predict the opponens move
17.4.2020       X predict a drawing probability directly
17.4.2020       - handle duplicate training positions
17.4.2020       - improve training window size handling

24.4.2020    Do a new training run to 99%+ and measure improvements made by these changes.

24.4.2020    The simple parts of the writing are completed: introduction, describe the known improvements, describe the algorithm, etc.

            Implement suggested new improvements, first the network architecture changes:
01.5.2020       - gather-excity blocks
01.5.2020       - mobilenetv3 network blocks

08.5.2020   Do a training run to 99%+ and measure improvements of the network architecture changes.

08.5.2020   Writing about proposed ideas is completed.

            Implement more conceptual changes to the algorithm:
22.5.2020       - Play games as trees and evaluate with a training run
05.6.2020       - Use internal network features of previous iterations as automatic auxilary targets and evaluate it with a training run
19.6.2020       - Adapt the self-playing phase to search for hyperpameters and evaluate it with a training run

23.6.2020   Write about measured results

            Implement optional known improvements:
26.6.2020        - Combine two networks in the search process and evaluate with a training run
03.7.2020        - Use the kullback-leibler distance to determine how certain the tree search is on a position and evaluate with a training run
                 - more optional stuff if time allows...

15.7.2020   Finalize writing            

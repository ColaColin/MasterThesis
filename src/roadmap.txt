Target      What
14.2.2020   Implement distributed training server and worker, so be ready to train at full "scale"

21.2.2020   Run first verification that training at full scale works up to a 99%+ perfect play on the testset.

28.2.2020   Time buffer to fix bugs and verify things really work.
            [X] try out weak2, does it reach the 4 correct answers per position?! Yes, it reaches 4.02, just shy of the 4.07 oracle devs stated.
            [X] change format of position database to human readable text, include game result
            [X] update the dataset tester for that
            [X] write unit tests for the above
            [X] supervised training scripts & direct network test script.
            [X] generate dataset that only takes one of the positions reached  per game to provide more data to the win-prediction.
            [X] update command/remote eval: Track only one dataset: Raw network moves, Raw network wins, Fully iterated policy moves. Rerun evaluation for old runs again...
            [X] insight into played games: Make them visible, including an insight into the network output and mcts results on them.
            [X] the broken win accuracy test code requires new supervised runs. Do 2 train window sizes only.

                [X] 2 block 1M
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  87.41  |  74.64  
           800000 |  89.02  |  75.98   <<< best

                [X] 5 block 1M
Training set size | moves % | wins %
------------------+---------+---------
           400000 |  90.10  |  76.32
           800000 |  91.63  |  77.47   <<< best

                [X] 5 block 1Mv2
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  91.12  |  78.18  
           800000 |  92.44  |  79.23   <<< best

                [X] 10 block 1M
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  90.80  |  76.63  
           800000 |  92.37  |  77.87   <<< best

                [X] 10 block 1Mv2
Training set size | moves % | wins %
------------------+---------+---------
           400000 |  91.82  |  78.66
           800000 |  93.00  |  79.67   <<< best

                [X] 20 block 1M
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  91.35  |  77.08  
           800000 |  92.98  |  78.23   <<< best

                [X] 20 block 1Mv2
Training set size | moves % | wins % 
------------------+---------+---------
           400000 |  92.12  |  78.90  
           800000 |  93.49  |  79.93   <<< best

            [X] do a hyperparameter search with a fixed network, fixed lr, etc. Search only 4 parameters with a 2 block network and 400 expansions: drawValue, cpuct, fpu, alphaBase. 3h runs, 50 iterations.
               -> searched parameters, sorted by accuarcy
N, acc, alphaBase, cpuct, drawValue, fpu
43,81.74,12.5,0.9267,0.4815,0.9168
62,81.61,20.38,1.545,0.6913,0.8545
23,81.3,12.55,0.8988,0.3037,1.0
49,81.18,12.57,0.8172,0.5063,1.0
57,81.12,16.13,0.7032,0.4977,0.7638
58,81.03,16.06,0.744,0.2199,0.8136
33,80.92,16.19,0.6191,0.3296,0.6402
55,80.9,12.65,1.13,0.04908,0.7947
39,80.86,12.7,1.049,0.3215,0.934
26,80.84,12.75,1.27,0.165,1.0
19,80.79,12.49,0.5677,0.1157,0.8806
28,80.74,12.65,0.7292,0.2831,0.8584
36,80.72,12.42,0.7851,0.2473,0.8453
27,80.7,12.68,0.5422,0.4029,0.7281
20,80.64,12.71,0.8139,0.000533,0.7598
59,80.59,16.27,0.7108,0.3427,0.8777
48,80.53,12.58,0.7973,0.5279,0.7396
35,80.52,20.69,1.502,0.7027,0.907
25,80.51,25.09,1.19,0.01503,0.161
34,80.51,12.62,0.4484,0.2873,0.8593
37,80.46,12.61,0.8516,0.09574,1.0
24,80.44,12.24,1.004,0.3101,0.8037
44,80.43,12.55,0.9343,0.2638,0.6285
64,80.42,20.38,1.811,0.8025,0.7701
30,80.41,12.86,1.028,0.002271,0.9282
53,80.35,16.25,0.7826,0.4555,0.5557
51,80.32,12.87,1.225,0.2191,0.7537
10,80.22,12.84,0.3788,0.1524,0.9715
11,80.12,29.87,2.12,0.8169,0.04463
45,80.11,14.38,2.654,0.8249,0.358
12,80.08,20.0,1.589,0.8373,0.8544
22,80.05,7.881,1.813,0.8035,0.648
29,80.04,12.39,1.072,0.2192,0.968
14,79.96,12.73,0.5443,0.4315,0.696
46,79.88,12.49,1.085,0.5019,0.8301
2,79.8,6.962,0.7809,0.1863,0.3456
15,79.76,19.79,1.537,0.8838,0.7593
61,79.74,15.93,0.6977,0.3855,0.6747
17,79.65,12.78,0.5301,0.4677,0.6818
52,79.64,16.16,0.4541,0.09906,0.6
16,79.58,3.256,3.414,0.162,0.4778
8,79.48,19.83,1.534,0.9591,0.8047
21,79.38,12.53,0.4773,0.0,0.5039
7,79.13,14.95,2.627,0.5478,0.7442
60,79.12,7.294,3.001,0.5067,0.1211
47,79.03,29.74,3.953,0.08781,0.07062
31,79.02,12.41,0.468,0.586,0.8486
63,78.99,16.24,0.5017,0.552,0.7515
18,78.96,9.32,2.82,0.5501,0.9492
5,78.84,14.27,3.462,0.1404,0.1981
3,78.71,13.71,3.348,0.4192,0.6852
54,78.56,10.74,0.8472,0.8536,0.1412
32,78.18,17.56,2.504,0.9372,0.4217
38,78.18,12.71,3.165,0.7248,0.5209
1,78.04,14.26,4.392,0.000114,0.3023
9,77.87,3.573,4.895,0.9649,0.7021
65,77.63,21.34,5.016,0.3277,0.3797
13,77.62,11.77,3.906,0.4005,0.7516
50,76.32,13.26,5.128,0.4109,0.3728
56,76.23,28.36,5.838,0.2482,0.1385
4,76.2,8.52,5.299,0.02739,0.6705
40,75.14,28.13,5.497,0.7549,0.6893
41,74.92,13.62,5.681,0.7649,0.7862
42,73.94,12.45,5.672,0.3793,0.2894
6,69.59,5.502,0.25,0.3374,0.03511

     -> the top ones are all very similar, with few exception, pick the two best, the best one represents a typical "best", the 2nd has an unusual cpuct value of above 1.5



               -> suggested parameters by prev work: drawValue 0.5, cpuct = 4, fpu = 0 (per deep mind), alphaBase = 7 (which yields alpha of 1, since alphaBase is divided by the number of legal moves)
            [X] Build a selfplay speed measurement system and run tests to show how much variation there is in the measurements.
            [X] Decide on the network size to use based on supervised results, 
            [X] Restart secure play workers
            [X] "Iteration 170% completed" -> why would it say that? Why not stop at 100%?!
            [X] raise for status and timeout for StatesDownloader2!
            [-] v2 of the training code should allow to restart the process  Skip, don't really need that.
            

            [X] do a baseline run. Those settings then also need to be what is "factor 1" of training speed. Whatever % that gets on 1Mv2 is the first training goal to reach faster...
               Goal to reach is 90.5% mcts accuracy. Measure cost to reach that iteration. Stop after 10 iterations with no further mcts accuracy improvement

               3 Sets of potential parameters were identified:
                    name      | alphaBase | cpuct  | drawValue | fpu
                    hyperopt1 | 12.5      | 0.9267 | 0.4815    | 0.9168
                    hyperopt2 | 20.38     | 1.545  | 0.6913    | 0.8545
                    prevWork  | 7         | 4      | 0.5       | 0

               [X] hyperopt1
                    [X] run1: 90.01% in iteration 22. Failed to reach 90.5%
                    [X] run2: 90.01% in iteration 25. Failed to reach 90.5%
                    [X] run3: 90.03% in iteration 20. Peak at 90.26% in iteration 25. Failed to reach 90.5%
                    [X] run4: 90.06% in iteration 20. Peak at 90.46% in iteration 33. Failed to reach 90.5%
                    [X] run5: 90.11% in iteration 19. Failed to reach 90.5%


               [X] hyperopt2
                    [X] run1: 90.5% in iteration 19. Peak 91% in iteration 33.
                    [X] run2: 90.41% in iteration 19. Failed to reach 90.5% :(
                    [X] run3: 90.07% in iteration 17. 90.51 in iteration 27.
                    [X] run4: 90.09% in iteration 19. Peak 91% in iteration 38.
                    [X] run5: 90.2% in iteration 19. Peak 90.7% in iteration 30.

               [X] prevWork
                    [X] run1: peak at 89.88% in iteration 49. 
                    [X] run2: peak at 89.76% in iteration 46.
                    [X] run3: peak at 89.86% in iteration 55.
                    [X] run4: peak at 90.24% in iteration 53.
                    [X] run5: peak at 89.95% in iteration 44.

               [~] run the cost evaluator

            [X] implement a backup script, which downloads all data from the server and restores it into my local database.

            - Allow to play vs any network on the web?
            - optimization: Insta-play forced moves without "thinking" at all? Do not produce frames for them?
                - what happens with such games currently anyway? What gets passed into the gpu for evaluation if there are no more nodes to expand?!
            - do this later, perf is ok for now: have a look at apex to use fp16 where possible
            - maybe have a relative playing strength comparision of the iterations?

20.4.2020   Figure out the best way to measure learning efficiency, as per chapter 4.1 of the proposal.

1.5.2020    The simple parts of the writing are completed: introduction, describe the known improvements, describe the algorithm, etc.

08.5.2020   Have a look at possible metrics to determine if the training suffers from any form of forgetting-and-relearning, as per chapter 4.2 of the proposal

            Implement known improvements that were stated as obligatory in the proposal:
15.5.2020       X more filters in the network heads -> implemented from the beginning
15.5.2020       X predict a drawing probability directly -> implemented from the beginning, because it makes the x0 framework more flexible
15.5.2020       X handle duplicate training positions
               [X] fix issue of "forgotten" frames by tracking which frames have been removed from the waitBuffer-cycle and readding them as new frames if they show up again
               [X] assert failed with a value of 0?!
               do a few test runs:
               [~] 0.2 weight: looks not much difference, though it failed on a bug in iteration 25.
               [X] 0.5 weight: peak 91.35% in iteration 37. 90% in 18, 91% in 28.
               [X] 0.8 weight: 90% in 16 <- probably a good idea to go with this one, 90% in 16 is pretty good

15.5.2020      [X] use a cyclic learning rate
               [X] run a test: looks good, seems to help over baseline
15.5.2020      [X] improve training window size handling
               [X] run a test: looks ok. No great improvement however.
                
15.5.2020      [X] use playout caps
               [X] run test: first test failed due to bad settings on the cheapMcts instance. It degraded game variety massively.
               -> more tests show either a "degrade game variety massively as it reaches 90.5% in good pace to then break off, or no improvements at all, depending on hyperparameter.
               -> it might be that 30 expansions is just not such a good idea?!
               -> or maybe playout cap randomization limits the final playing strength due to less higher quality games being played.
               -> this could be a hyperparameter problem, or connect4 just doesn't suffer as much from the issue as go does, so playout caps do not help for connect4?
               -> for now skip using this. Maybe later try a few more things:
               [ ] one more run: p = 50%, cheapSearch does 100 expansions.
               [ ] implement a "NoOpPolicyIterator", which just forwards through the network once and returns the network output directly
                    -> maybe do not use randomization to decide which position is fully analyzed, instead do it on positions where the first network output is not "obvious", e.g. has a high entrophy.
                    -> this would be my own extension, so would belong into the "my own extensions"-part
15.5.2020      [X] squeeze-and-excitation networks
               [X] run supervised training to test how much better this kind of network can fit connect4 data.

Training set size | moves % | wins % 
------------------+---------+---------
           800000 |  92.71  |  79.44   <<< best

               [X] do a test run -> 90% in iteration 16, which is maybe about the cost of iteration 17 without sq, so pretty good. Though it then failed to reach 90.5%...
15.5.2020       - predict the opponens move
               [X] implement this
               [X] test run: 90% in iteration 17. Seems ok


22.5.2020    Do a new training run to 99%+ and measure improvements made by these changes.
          [X] rainbow run: everything, but playout caps.

               [X] run1
               [X] run2
               [X] run3
               [X] run4
               [X] run5

-> Revised by weeks:
WK 23
     [X] complete extended run
     [X] write some code that makes diagrams from the data on the server to be included into latex
     [X] self-play hyperparameter search, possibly with dynamic number of expansions decided by a very simple secondary network that is evolved by self-play
     -> full blown run (league_test_2): issues with lack of game variance, bad results
     -> test_3: tried some different parameter ranges, did not help much
     -> test_4: only mcts parameters: just as bad, quick drop of variance as well. Variance likely an issue of cpuct
     -> test_5: use only mcts parameters, only small range around known good params: good results
     -> test_6: good mcts, free thinking time: bad, but not as bad
     -> test_7 good mcts, limited thinking time params: still not great, definitely not an improvement? (unless framestime are much lower?)
     -> or maybe everything above is invalid, as the -100 rating on mutated players for the most part removes them from the league the moment the are created?!

     [X] frametime evaluator needs to configure the worker to use player-configs that existed at the time of the network measured using the matchistory, probably needs an extra api.
     -> select all player configs which played at least one game with a given network and only use those to play.
WK 24
     [X] league generation changes: New generation when the players of the last new generation have played X games.
     [X] best players for a network to be defined as the best players of the last generation to only play games with that network
          X record a "generation"-number for the players to implement this.
          X remote_eval only evalutates networks for which the latest generation is completed, e.g. there has to be a newer generation which is playing with some other network now.
     [?] fix the frametime evaluation of league-based runs, somehow they are broken, don't play with the right number of nodes.
     [~] run more experiments with evolving players
          -> just evolving drawValue and fpu without restrictions is pretty bad

          Key questions to research on "can this work at all":
          -> Does the evolution find good players in the presence of the exploration noise?
          -> Does a "good" player correspond to a high accuracy according to the solver?
               -> compare MCTS accuracy of different players as a chart of mcts accuarcy over rank in the ladder after a run for the top 50 players.
          -> What about game variance, will that break down again?

          -> Question: Does the ladder-based evolution pick out strong players at all? Or is it all just random noise?
               - use a "inversion"-hyperparameter, where 1 means "play bad moves intentionally" and 0 "play the good moves". I.e.: it should optimize towards 0, if the optimization works.
               - test if if works with restricted evolution

new mutated players:
x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '95630379-d5fa-4c3c-af4d-beaacdeebbad' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.532083449089683 | 0.270653516326751
          2 | 0.375197814043052 | 0.609621298291894
          3 | 0.147329403989332 | 0.350569696805265
          4 | 0.304481757692259 | 0.992657085621371
          5 | 0.186916487718976 | 0.845452319314785
          6 | 0.138749106244541 | 0.943134779230976
          7 | 0.142371928611014 | 0.823057518836132
          8 | 0.151204016447625 |  1.69328668444003
          9 | 0.245806452288956 |  1.22883539299929
         10 | 0.201451812705329 |  1.84845647237876
         11 |  0.16890345582608 |  1.57438121601208
         12 | 0.134046040254104 |  2.58304425784389
         13 | 0.357497021388468 |  3.89792466206146
(13 rows)

best players:
x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '95630379-d5fa-4c3c-af4d-beaacdeebbad' group by s.generation;
 generation |         avg         
------------+---------------------
          3 |   0.108627227686143
          4 |   0.071396162599138
          5 |  0.0235274631539082
          6 |  0.0122366837273928
          7 |  0.0108890069930788
          8 | 0.00653001957834964
          9 | 0.00244810817886486
         10 | 0.00280159160851841
         11 | 0.00367794678745987
         12 | 0.00303961886623186
(10 rows)

restricted evolution causes the stddev to explode, can that be prevented by looping restricted values? That should given an evolutionary advantage to small stddev values instead of big ones?

               -> do a test with chances 100%, 25%, 5% of inversion based on the selected value, how reliable does it find the better players for those cases? The lower it goes, the more reliable even small differences between players will be found.
                    -> 10 generation each, avg of results from 3 runs.

100%:


b5da9c0b-4db1-49a0-b5aa-8d824e8de1f7
 generation |        avg
------------+--------------------
          3 |  0.135315670930277
          4 | 0.0776798559955779
          5 | 0.0658959418103126
          6 |  0.039462156116895
          7 | 0.0401443461050646
          8 | 0.0329108122074383
          9 | 0.0222988996862122
         10 |  0.024455258842138

 generation |        avg        |        avg         
------------+-------------------+--------------------
          1 | 0.474337871731481 |  0.277312684498163
          2 | 0.358937621854321 |  0.630670298570815
          3 | 0.386100913086458 |  0.544380223157011
          4 | 0.239808859000899 |  0.229556865807236
          5 | 0.145741467295021 | 0.0563991155439515
          6 | 0.342233843015984 |   0.19255911746328
          7 | 0.307207798753939 | 0.0838927633805477
          8 | 0.347439185998696 |  0.128789142140572
          9 | 0.269315758544495 | 0.0732674588901536
         10 | 0.268842143493111 |  0.509530181871259
         11 | 0.281735015468705 | 0.0726444064916898

e104cee9-ee66-4328-8a3d-cc47c0feeb33
x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = 'e104cee9-ee66-4328-8a3d-cc47c0feeb33' group by generation order by generation asc;
 generation |        avg        |        avg         
------------+-------------------+--------------------
          1 | 0.497032953958438 |  0.251886989394464
          2 | 0.365818711912343 |  0.485207308745577
          3 | 0.393445476948225 |  0.391144245495933
          4 |     0.27234581778 |  0.409842774967983
          5 | 0.258452504284082 |  0.789533471319203
          6 | 0.229666889326379 |   1.02225254739313
          7 | 0.246915366241892 |    1.6777226727363
          8 | 0.202325767362225 | 0.0751656866565498
          9 | 0.195413279844559 | 0.0474251141339001
         10 | 0.397948313005969 |  0.161716502899679
         11 | 0.209807663659014 |  0.044573453919493
(11 rows)

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = 'e104cee9-ee66-4328-8a3d-cc47c0feeb33' group by s.generation;
 generation |        avg         
------------+--------------------
          3 |    0.1570463880953
          4 | 0.0877856938890719
          5 | 0.0781986599772075
          6 | 0.0527047724425045
          7 |  0.051425144747466
          8 | 0.0440569015452784
          9 | 0.0302993221776173
         10 | 0.0302047049518767
(8 rows)



1e3bfeb2-18dc-447a-a1e0-e3a423cc8d3c
x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '1e3bfeb2-18dc-447a-a1e0-e3a423cc8d3c' group by generation order by generation asc;
 generation |        avg         |        avg         
------------+--------------------+--------------------
          1 |  0.504080716786349 |  0.265643382418339
          2 |  0.350329631638036 |   0.39166183668799
          3 |  0.406854135941621 |  0.415718120908287
          4 |  0.152844698755316 | 0.0737823579027844
          5 |  0.165884066615793 | 0.0559317034144853
          6 |  0.120029678184099 |  0.101869306897708
          7 | 0.0859888069207291 | 0.0304126168697269
          8 |  0.111539889459197 | 0.0409149665126872
          9 |  0.192367672391646 | 0.0968860125251314
         10 | 0.0883713203315914 | 0.0545262378403878
         11 |  0.216369814380291 | 0.0151219499964586
(11 rows)

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '1e3bfeb2-18dc-447a-a1e0-e3a423cc8d3c' group by s.generation;
 generation |        avg         
------------+--------------------
          3 |  0.154824406054617
          4 | 0.0860682647905006
          5 | 0.0684224340217999
          6 | 0.0627801693298965
          7 | 0.0500601891415002
          8 | 0.0376065575896017
          9 | 0.0330803049687997
         10 | 0.0262694430357088
(8 rows)


25%

cbbcb00a-b86e-4c5b-9500-ded1f27166c4

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = 'cbbcb00a-b86e-4c5b-9500-ded1f27166c4' group by s.generation;
 generation |        avg
------------+--------------------
          3 |  0.212170113001941
          4 |   0.20596946524389
          5 |  0.148560485279079
          6 |  0.119348676934505
          7 | 0.0808065710691078
          8 | 0.0823343761000614
          9 | 0.0716461937840111
         10 | 0.0739518568959384
(8 rows)

x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = 'cbbcb00a-b86e-4c5b-9500-ded1f27166c4' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.384153554324508 | 0.251442854182465
          2 | 0.425582881557033 | 0.369404085712966
          3 | 0.432255795764667 |  0.58656131485665
          4 | 0.462092670976575 | 0.495578017079631
          5 |  0.33641568338584 | 0.188809665963335
          6 | 0.328765795453694 | 0.324661765659182
          7 | 0.325066156446237 | 0.934821471754356
          8 | 0.365700865651259 | 0.200254720102156
          9 | 0.295305938662884 | 0.177940645977792
         10 | 0.333849323795964 | 0.142599275284825
         11 | 0.242002368336345 | 0.148256106971585
(11 rows)



33d4aff2-887d-4881-8f06-0dbf26e60a59

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '33d4aff2-887d-4881-8f06-0dbf26e60a59' group by s.generation;
 generation |        avg
------------+--------------------
          3 |  0.295103006304245
          4 |  0.205320768916679
          5 |  0.184220516266129
          6 |   0.15110185292731
          7 |   0.12349689360056
          8 |  0.116973748631245
          9 |  0.100577539219479
         10 | 0.0879656940967953
(8 rows)

x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '33d4aff2-887d-4881-8f06-0dbf26e60a59' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.472563391416715 | 0.224174708641528
          2 | 0.422353879507677 | 0.403377854726382
          3 | 0.431603446575304 | 0.338591254451142
          4 | 0.461684452021634 | 0.880789409116457
          5 | 0.284969104577176 |  0.35075569664648
          6 | 0.447431691449873 | 0.562346709034966
          7 | 0.332079620082613 | 0.347776797330093
          8 | 0.298899161714035 |  0.37310604249274
          9 | 0.291883528254149 | 0.864504630052536
         10 | 0.331702565111888 | 0.413408011178298
         11 | 0.337580018577981 | 0.161491387294501
(11 rows)


97332981-fd25-483d-9bad-9d036815a5ff

x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '97332981-fd25-483d-9bad-9d036815a5ff' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.513161711766669 | 0.232987810379259
          2 | 0.419422365024771 | 0.349745860162608
          3 | 0.509121764898086 | 0.599408936939126
          4 | 0.449653839618896 | 0.519066499743236
          5 | 0.416748091862163 | 0.597959122261617
          6 | 0.395693914970587 | 0.556768624806731
          7 | 0.365015458708107 | 0.662538942593491
          8 |  0.29701022053106 |  0.50969336166287
          9 | 0.306398336298672 | 0.368212389327364
         10 | 0.306941297924904 | 0.621599906462538
         11 | 0.263873427327939 | 0.378667112683685
(11 rows)

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '97332981-fd25-483d-9bad-9d036815a5ff' group by s.generation;
 generation |        avg
------------+--------------------
          3 |  0.301175329788254
          4 |  0.240982365501921
          5 |  0.182945310125873
          6 |    0.1809141262974
          7 |  0.123343664346808
          8 |  0.119950059260782
          9 |  0.101287994628267
         10 | 0.0943173777349969
(8 rows)



5%

6ff71176-a20f-4373-a261-e365e1c70471


x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '6ff71176-a20f-4373-a261-e365e1c70471' group by s.generation;
 generation |        avg
------------+-------------------
          4 | 0.360127060234895
          5 | 0.384013603676988
          6 | 0.297299305574522
          7 | 0.292279478595817
          8 | 0.271969061408263
          9 | 0.253473921671012
         10 | 0.212513869517923
(7 rows)



x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '6ff71176-a20f-4373-a261-e365e1c70471' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.466175743393894 | 0.236445689276487
          2 | 0.437295782429233 | 0.325522426160383
          3 | 0.526922354319683 | 0.214462653502596
          4 | 0.370265646493185 | 0.463718527262305
          5 | 0.465670579342031 | 0.251888716391242
          6 | 0.390284620899183 | 0.320643251561618
          7 | 0.503366989071182 | 0.830855038856009
          8 | 0.352115016405858 | 0.340915194047769
          9 | 0.383152101321452 | 0.222487266876839
         10 | 0.332324822973722 | 0.522030107516732
         11 | 0.230733535560882 | 0.150550303103431
(11 rows)


53510cf3-93e0-49a0-80be-4581f88eb162

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '53510cf3-93e0-49a0-80be-4581f88eb162' group by s.generation;
 generation |        avg
------------+-------------------
          3 | 0.467040313780731
          4 | 0.478016009158946
          5 | 0.418297385852119
          6 |   0.3524033635016
          7 | 0.333685482984613
          8 |  0.24412976762613
          9 |  0.24307571129686
         10 | 0.285478073211543
         11 |  0.21846347535757
(9 rows)

x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '53510cf3-93e0-49a0-80be-4581f88eb162' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.488834254843892 | 0.264639645517322
          2 | 0.489185821699346 | 0.444498674011574
          3 | 0.478590151245306 | 0.801285741030059
          4 | 0.510468051218407 | 0.758257851463733
          5 | 0.512760186242474 | 0.780650675224374
          6 | 0.420453472297268 |  2.26619683541688
          7 | 0.470798373750446 |  2.65750675490895
          8 | 0.432507395377698 |  1.93072717635557
          9 | 0.420868531166156 |  1.24385233929934
         10 | 0.417166335365749 |  1.70685108067643
         11 | 0.309617375723279 | 0.457979408740548
         12 | 0.303508832032863 | 0.582573480183841
(12 rows)


2c02e417-0945-43eb-b45d-9462d4000f23

x0=> select s.generation,  avg((p.parameter_vals::json -> 'inversion')::text::float) from league_players_snapshot s, networks n, league_players p where p.id = s.id and s.network = n.id and n.run = '2c02e417-0945-43eb-b45d-9462d4000f23' group by s.generation;
 generation |        avg
------------+-------------------
          3 | 0.403892524517762
          4 | 0.303165098415021
          5 | 0.345845439324263
          6 | 0.272767889649582
          7 | 0.244265420821731
          8 | 0.216503816680334
          9 | 0.231079060897481
         10 | 0.246223168099894
(8 rows)

x0=> select generation, avg((parameter_vals::json -> 'inversion')::text::float), avg((parameter_stddevs::json -> 'inversion')::text::float) from league_players where run = '2c02e417-0945-43eb-b45d-9462d4000f23' group by generation order by generation asc;
 generation |        avg        |        avg
------------+-------------------+-------------------
          1 | 0.518932527630487 | 0.238609544433569
          2 | 0.364211899651556 | 0.560477600304842
          3 | 0.490276821952013 | 0.328678356698229
          4 | 0.295060300507853 |  0.35078630544329
          5 | 0.479829340036555 | 0.250291297994818
          6 | 0.414649069256172 | 0.248934993978178
          7 |  0.41475838355093 |   0.2647510782443
          8 | 0.366573197715568 | 0.408978730113543
          9 | 0.427299168011638 | 0.275357812014702
         10 | 0.331852592932152 | 0.166616900350705
         11 | 0.426943379280087 | 0.242017361540383
(11 rows)



     [~] write more about the basic setup of the experiments, the way accuracy is measured, the way costs are measured.
     [~] writing goal: Write about the hyperparameter search
     [X] league run: Simple parameters, limited.
          -> first few iterations learn faster, at least in this one attempt, but then it starts to suffer from less game variance and falls behind.
WK 25
     [ ] more work on the league system, investigate why it fails (i.e. that game variance issue?)
          -> game variance is very visible and probably part of the reason it does not work
          -> so how well do the selected parameters do vs the standard extended parameters in pure MCTS accuracy using the same network?
               -> write code to compare different parametersets on the same network and evaluation set. Compare best found players vs "extended" player
                    -> Then do a hyperparameter search in this way via bayesian optimization (could do the entire thing in a hyperopt script, just use best found players and extended as initial points?!)
                    on run: https://x0.cclausen.eu/#runs/list/325d9f51-97d2-48ab-8999-25f2583979ba
                    [X] run on an early network, e.g. iteration 1
               acc      | cpuct     | drawValue | fpu
|  1        |  0.7415   |  0.5328   |  0.8753   |  0.9912   | <<<<<<< best league player
|  2        |  0.7512   |  1.545    |  0.6913   |  0.8545   | <<<<<<< extended
|  27       |  0.7613   |  0.8162   |  1.0      |  0.5255   | <<<<<<<< best

                    -> evolved player does not appear to be particulary good in terms of accuracy. Might be due to the fact that the first network also had not that many generations yet.
                    -> hyperopt only found one configuration substantially better than extended

                    [X] run on the best network
                         iteration 24 is the best network based on network accuracy

                         -> it appears the players league yields a player about as strong as the extended baseline, so a winning player correlates with a players for decent progress.
                         -> looking closer the best players reach 91%+ with a cpuct a bit lower than extended, but with a 0 fpu.
                         -> so the extended player might be a compromise, high fpu hurts its performance at this point of the training.
                         -> The league does find a lower fpu, so that is why it can compete with extended maybe?
                         -> Maybe high fpu is good at the start of training?

            | acc       | cpuct     | drawValue | fpu
|  1        |  0.9068   |  0.2925   |  0.4728   |  0.1411   | <<<<<<<<<< players league: 90.68%
|  2        |  0.906    |  1.545    |  0.6913   |  0.8545   | <<<<<<<<<< extended baseline: 90.6%
|  30       |  0.9117   |  0.9045   |  0.4498   |  0.000977 |  <<<<<<<<<<<<<<<<< best: 91.17%!



                    [X] run on a random network
                         -> no league player, as none gets recorded
                         -> extended is far from optimal, best players highlight: low cpuct and very high fpu is a good idea with a random network.
                         -> makes sense: FPU = 1 provides a decent exploration to find "easy wins"

|  1        |  0.6227   |  1.545    |  0.6913   |  0.8545   |       <<<<<<< extended     
|  48       |  0.6712   |  0.08295  |  0.106    |  0.9995   |  <<<<<<<<<<<< best player

                    [X] write code to let players play some games against each other, given a run & network ofc
                         -> do the hyperopt players with the max. accuracy win the most games? Or is there more to it than that?
                         -> If yes: evolution is just not good enough, do one more try to improve the evolution part (research some other evolution algo, bigger population, glicko rating)
                         -> If no: winning against another player using the same network is not just about playing good, there are other effects at play, that are not that useful as learning signal for the evolution?

                    [X] evaluation of player strength, iteration 1
{('evolved', 'extended'): (665, 291, 44), ('evolved', 'hyperopt'): (526, 380, 94), ('extended', 'hyperopt'): (366, 581, 53)}
-> evolved wins against all other settings, so just winning games at the current level is not necessarily good for mcts accuracy and general training progress.

                    [X] evaluation of player strength, iteration 24
{('evolved', 'extended'): (557, 155, 288), ('evolved', 'hyperopt'): (422, 388, 190), ('extended', 'hyperopt'): (238, 590, 172)}
-> again evolved wins against everything, even though mcts accuracy does not show it directly.

                    [X] evaluation of player strength, iteration 0
{('extended', 'hyperopt'): (360, 625, 15)}
-> no evolved player available here, but hyperopt clearly defeats extended, showing that a low cpuct indeed is better on a random network.

-> conclusion: Just winning more games against other versions of the network at slightly different parameters does not mean higher mcts accuracy, e.g. does not mean training progress happens faster. Ups!

                    [X] test out dynamic number of evaluations again, some tweaks before that to make the kl div implementation cleaner. Maybe that'll help...
                         -> "MEH"

     [ ] two little tweaks inbetween:
          [~] renormalize edgePriors to 1 with respect to legal moves.
          [ ] the dynamic fpu thing.

     [ ] writing goal: Complete stuff about the extended run
WK 26
     [ ] buffer
WK 27
     [ ] play games as mcts-like-trees
     [ ] writing goal: Complete stuff about the hyperparameter search
WK 28
     [ ] buffer
WK 29
     [ ] reuse internal network features
     [ ] writing goal: Complete stuff about the tree-game-playing
WK 30: 
     [ ] buffer
WK 31:
     [ ] document the code
     [ ] writing goal: Complete stuff about the internal network features

WK 32:
     [ ] writing goal: Complete writing
     [ ] prepare presentation
WK 33:
     [ ] writing goal: conclusions
     [ ] hold presentation

WK 34: deadline on Thursday

     Skip it?:
     [?] implement network architecture changes: gather-excite and mobilenetv3
          -> maybe skip this, the improvements from squeeze-excite where already quite small, gather-excite should be an even smaller step up. Not worth the time.

     [ ] use the thinkdecider in the evaluation code by estimating the average number of remaining moves based on the turn number.

     Implement suggested new improvements, first the network architecture changes:
       - gather-excity blocks
       - mobilenetv3 network blocks

   Do a training run to 99%+ and measure improvements of the network architecture changes.

   Writing about proposed ideas is completed.

            Implement more conceptual changes to the algorithm:
       - Adapt the self-playing phase to search for hyperpameters and evaluate it with a training run
       - Play games as trees and evaluate with a training run
       - Use internal network features of previous iterations as automatic auxilary targets and evaluate it with a training run

   Write about measured results

            Implement optional known improvements:
        - Combine two networks in the search process and evaluate with a training run
        - Use the kullback-leibler distance to determine how certain the tree search is on a position and evaluate with a training run
                 - more optional stuff if time allows...

   Finalize writing            

Some extra ideas:
     - exploration is really stupid in the lategame, try out to reduce temperature by ply 12 instead of 30
     - the fpu calculation based on "Am I winning?"
     - "if I am winning, do not explore at all"
     - play out forced moves automatically without any policy iteration on them. Do not record them as training frames.
          -> this also would need to happen inside the mcts tree?!






players hyperopt:

                    on run: https://x0.cclausen.eu/#runs/list/325d9f51-97d2-48ab-8999-25f2583979ba
                    [X] run on an early network, e.g. iteration 1
|  1        |  0.7415   |  0.5328   |  0.8753   |  0.9912   | <<<<<<< best league player
|  2        |  0.7512   |  1.545    |  0.6913   |  0.8545   | <<<<<<< extended
|  3        |  0.7427   |  2.502    |  0.7203   |  0.000114 |
|  4        |  0.7491   |  1.814    |  0.1468   |  0.09234  |
|  5        |  0.7532   |  1.118    |  0.3456   |  0.3968   |
|  6        |  0.7389   |  3.233    |  0.4192   |  0.6852   |
|  7        |  0.75     |  1.227    |  0.8781   |  0.02739  |
|  8        |  0.7339   |  4.023    |  0.4173   |  0.5587   |
|  9        |  0.7498   |  0.8423   |  0.1981   |  0.8007   |
|  10       |  0.725    |  5.81     |  0.3134   |  0.6923   |
|  11       |  0.725    |  6.0      |  1.0      |  1.0      |
|  12       |  0.2531   |  0.0      |  7.569e-0 |  0.0      |
|  13       |  0.7215   |  6.0      |  1.0      |  0.0      |
|  14       |  0.7355   |  3.452    |  0.01583  |  0.000591 |
|  15       |  0.7311   |  4.522    |  0.000558 |  0.9936   |
|  16       |  0.7271   |  4.718    |  1.0      |  0.0      |
|  17       |  0.7228   |  6.0      |  0.0      |  0.0      |
|  18       |  0.7471   |  1.921    |  0.0      |  1.0      |
|  19       |  0.7323   |  4.571    |  1.0      |  1.0      |
|  20       |  0.7273   |  5.007    |  0.0      |  0.0      |
|  21       |  0.7358   |  3.483    |  1.0      |  0.0      |
|  22       |  0.7462   |  1.554    |  0.9995   |  0.3566   |
|  23       |  0.7244   |  6.0      |  0.0      |  1.0      |
|  24       |  0.7472   |  1.123    |  0.3489   |  0.9996   |
|  25       |  0.7373   |  3.209    |  0.9843   |  0.9831   |
|  26       |  0.7454   |  2.48     |  0.0      |  0.427    |
|  27       |  0.7613   |  0.8162   |  1.0      |  0.5255   | <<<<<<<< best
|  28       |  0.737    |  3.313    |  0.05613  |  0.9984   |
|  29       |  0.7474   |  1.003    |  0.9888   |  0.9491   |
|  30       |  0.7437   |  2.447    |  0.442    |  1.0      |
|  31       |  0.751    |  1.806    |  0.3464   |  0.5345   |
|  32       |  0.7575   |  1.025    |  0.7245   |  0.65     |
|  33       |  0.7546   |  1.275    |  0.002925 |  0.7261   |
|  34       |  0.7461   |  2.07     |  0.988    |  0.9912   |
|  35       |  0.7267   |  5.285    |  0.9987   |  0.524    |
|  36       |  0.7274   |  5.214    |  0.4547   |  0.9806   |
|  37       |  0.7429   |  2.711    |  0.01925  |  0.02541  |
|  38       |  0.7421   |  2.697    |  0.997    |  0.5054   |
|  39       |  0.7506   |  1.115    |  0.9931   |  0.3961   |
|  40       |  0.7508   |  1.738    |  1.0      |  1.209e-0 |
|  41       |  0.7506   |  1.628    |  0.5934   |  0.01981  |
|  42       |  0.7552   |  0.5019   |  0.9909   |  0.5638   |
|  43       |  0.7237   |  5.468    |  0.5214   |  0.000480 |
|  44       |  0.7474   |  0.6946   |  0.9855   |  0.7472   |
|  45       |  0.7213   |  0.6391   |  0.9969   |  0.01443  |
|  46       |  0.6527   |  0.0      |  0.8871   |  1.0      |
|  47       |  0.7349   |  3.944    |  0.9867   |  0.5353   |
|  48       |  0.7304   |  4.239    |  0.0      |  3.168e-1 |
|  49       |  0.7547   |  0.7416   |  0.7068   |  0.4666   |
|  50       |  0.7359   |  3.823    |  0.6985   |  0.9908   |
|  51       |  0.7272   |  5.374    |  0.002479 |  0.6497   |
|  52       |  0.7496   |  1.687    |  0.003826 |  0.4471   |
|  53       |  0.7294   |  4.055    |  0.5472   |  0.004748 |
|  54       |  0.7479   |  1.013    |  0.03965  |  0.9906   |
|  55       |  0.7423   |  2.617    |  7.777e-0 |  0.9152   |
|  56       |  0.7544   |  0.7204   |  0.9942   |  0.3276   |
|  57       |  0.7313   |  4.742    |  0.4653   |  0.4466   |
|  58       |  0.7533   |  1.338    |  0.3813   |  0.5741   |
|  59       |  0.7481   |  2.144    |  0.7519   |  0.5238   |
|  60       |  0.7482   |  2.197    |  0.006443 |  0.03117  |



                    [X] run on the best network
                         iteration 24 is the best network based on network accuracy

                         -> it appears the players league yields a player about as strong as the extended baseline, so a winning player correlates with a players for decent progress.
                         -> looking closer the best players reach 91%+ with a cpuct a bit lower than extended, but with a 0 fpu.
                         -> so the extended player might be a compromise, high fpu hurts its performance at this point of the training.
                         -> The league does find a lower fpu, so that is why it can compete with extended maybe?
                         -> Maybe high fpu is good at the start of training?

|  1        |  0.9068   |  0.2925   |  0.4728   |  0.1411   | <<<<<<<<<< players league: 90.68%
|  2        |  0.906    |  1.545    |  0.6913   |  0.8545   | <<<<<<<<<< extended baseline: 90.6%
|  3        |  0.9061   |  2.502    |  0.7203   |  0.000114 | 
|  4        |  0.9058   |  1.814    |  0.1468   |  0.09234  |
|  5        |  0.9093   |  1.118    |  0.3456   |  0.3968   |
|  6        |  0.9021   |  3.233    |  0.4192   |  0.6852   |
|  7        |  0.9075   |  1.227    |  0.8781   |  0.02739  |
|  8        |  0.9002   |  4.023    |  0.4173   |  0.5587   |
|  9        |  0.9052   |  0.8423   |  0.1981   |  0.8007   |
|  10       |  0.8964   |  5.81     |  0.3134   |  0.6923   |
|  11       |  0.8977   |  0.04609  |  0.9822   |  0.04767  |
|  12       |  0.8965   |  5.962    |  1.0      |  1.0      |
|  13       |  0.8973   |  5.991    |  0.9735   |  0.07736  |
|  14       |  0.8982   |  0.1135   |  0.01903  |  0.01899  |
|  15       |  0.9073   |  0.4691   |  0.9489   |  0.9884   |
|  16       |  0.9096   |  0.9595   |  0.7511   |  0.5044   |
|  17       |  0.9101   |  0.979    |  0.639    |  0.4105   |
|  18       |  0.906    |  1.142    |  0.9742   |  0.5568   |
|  19       |  0.9025   |  0.01694  |  0.419    |  0.9263   |
|  20       |  0.9104   |  1.155    |  0.5016   |  0.01924  |
|  21       |  0.9109   |  1.07     |  0.4792   |  0.01948  |
|  22       |  0.9114   |  1.131    |  0.4887   |  0.004447 |
|  23       |  0.9108   |  1.148    |  0.5151   |  0.007308 |
|  24       |  0.9108   |  1.179    |  0.4576   |  0.003849 |
|  25       |  0.9114   |  0.981    |  0.545    |  0.002272 |
|  26       |  0.911    |  1.047    |  0.4205   |  0.003497 |
|  27       |  0.9112   |  1.016    |  0.5049   |  3.72e-05 |
|  28       |  0.9102   |  1.223    |  0.4373   |  0.00746  |
|  29       |  0.9114   |  0.8715   |  0.4123   |  0.000405 |
|  30       |  0.9117   |  0.9045   |  0.4498   |  0.000977 |  <<<<<<<<<<<<<<<<< best: 91.17%!
|  31       |  0.9025   |  3.511    |  0.1238   |  0.007948 |
|  32       |  0.9115   |  0.9027   |  0.4888   |  0.01413  |
|  33       |  0.9113   |  0.99     |  0.4651   |  0.01148  |
|  34       |  0.9111   |  0.9749   |  0.4242   |  0.001321 |
|  35       |  0.9113   |  0.898    |  0.5201   |  0.01475  |
|  36       |  0.9112   |  0.9443   |  0.543    |  0.008903 |
|  37       |  0.9107   |  1.001    |  0.5146   |  0.007127 |
|  38       |  0.9111   |  0.8833   |  0.4537   |  0.003767 |
|  39       |  0.9109   |  1.048    |  0.3842   |  0.007543 |
|  40       |  0.9106   |  0.9084   |  0.5355   |  0.2495   |
|  41       |  0.9113   |  1.025    |  0.4959   |  0.000736 |
|  42       |  0.9108   |  0.9896   |  0.4738   |  0.002852 |
|  43       |  0.9107   |  1.098    |  0.3938   |  0.01374  |
|  44       |  0.9116   |  0.8392   |  0.5267   |  0.003709 |
|  45       |  0.9104   |  0.8445   |  0.5042   |  0.2356   |
|  46       |  0.9114   |  0.8775   |  0.4421   |  0.006849 |
|  47       |  0.9112   |  0.9466   |  0.5534   |  0.006821 |
|  48       |  0.9108   |  1.121    |  0.5091   |  0.004508 |
|  49       |  0.9117   |  0.9084   |  0.5425   |  0.01435  |
|  50       |  0.911    |  1.03     |  0.4879   |  0.004557 |
|  51       |  0.9111   |  0.9621   |  0.4148   |  0.006293 |
|  52       |  0.9104   |  0.9475   |  0.4657   |  0.00466  |
|  53       |  0.9113   |  1.071    |  0.4943   |  0.000958 |
|  54       |  0.9114   |  0.8216   |  0.5428   |  0.002533 |
|  55       |  0.9114   |  0.8374   |  0.562    |  0.01882  |
|  56       |  0.9114   |  0.9067   |  0.4591   |  0.007124 |
|  57       |  0.9111   |  0.9583   |  0.57     |  0.01082  |
|  58       |  0.9116   |  0.9062   |  0.4776   |  0.00602  |
|  59       |  0.911    |  0.9422   |  0.4355   |  0.004783 |
|  60       |  0.9113   |  0.8322   |  0.4518   |  0.001593 |


random network player hyperopt:
|  1        |  0.6227   |  1.545    |  0.6913   |  0.8545   |       <<<<<<< extended                                                                                                                                                                                                               [57/239]
|  2        |  0.5512   |  2.502    |  0.7203   |  0.000114 |
|  3        |  0.5745   |  1.814    |  0.1468   |  0.09234  |
|  4        |  0.6266   |  1.118    |  0.3456   |  0.3968   |
|  5        |  0.613    |  3.233    |  0.4192   |  0.6852   |
|  6        |  0.5573   |  1.227    |  0.8781   |  0.02739  |
|  7        |  0.6044   |  4.023    |  0.4173   |  0.5587   |
|  8        |  0.6278   |  0.8423   |  0.1981   |  0.8007   |
|  9        |  0.5689   |  5.81     |  0.3134   |  0.6923   |
|  10       |  0.5311   |  6.0      |  1.0      |  1.0      |
|  11       |  0.5475   |  6.0      |  1.0      |  0.0      |
|  12       |  0.1784   |  0.02412  |  0.05351  |  0.01913  |
|  13       |  0.5726   |  4.044    |  1.0      |  1.0      |
|  14       |  0.6203   |  1.787    |  0.0      |  1.0      |
|  15       |  0.5611   |  6.0      |  0.0      |  0.0      |
|  16       |  0.6076   |  4.319    |  0.0      |  1.0      |
|  17       |  0.6426   |  0.2899   |  1.0      |  1.0      |
|  18       |  0.5916   |  4.757    |  0.0      |  0.0      |
|  19       |  0.6303   |  0.8484   |  1.0      |  1.0      |
|  20       |  0.5646   |  4.655    |  1.0      |  0.0      |
|  21       |  0.5949   |  3.483    |  0.0      |  0.0      |
|  22       |  0.6279   |  0.8822   |  0.5418   |  1.0      |
|  23       |  0.6202   |  2.624    |  0.0      |  0.6107   |
|  24       |  0.6331   |  0.5916   |  1.0      |  0.6624   |
|  25       |  0.6218   |  1.327    |  1.726e-0 |  0.7174   |
|  26       |  0.6134   |  3.334    |  0.0      |  1.0      |
|  27       |  0.5596   |  5.962    |  0.009825 |  0.9538   |
|  28       |  0.609    |  2.548    |  1.0      |  1.0      |
|  29       |  0.6146   |  2.529    |  0.4365   |  0.9919   |
|  30       |  0.6264   |  1.1      |  0.6885   |  0.6165   |
|  31       |  0.5972   |  5.005    |  0.0      |  0.6857   |
|  32       |  0.6134   |  3.747    |  0.0      |  0.5729   |
|  33       |  0.6338   |  0.5613   |  0.9917   |  0.9741   |
|  34       |  0.5673   |  3.666    |  0.9983   |  0.1122   |
|  35       |  0.6081   |  0.01247  |  0.9262   |  0.8911   |
|  36       |  0.6226   |  1.193    |  0.2249   |  0.9762   |
|  37       |  0.6181   |  1.578    |  1.0      |  1.0      |
|  38       |  0.5814   |  5.055    |  0.493    |  0.9977   |
|  39       |  0.6228   |  1.942    |  0.2481   |  0.6966   |
|  40       |  0.6272   |  1.005    |  0.4037   |  0.7094   |
|  41       |  0.6337   |  0.3877   |  0.1705   |  0.9951   |
|  42       |  0.6282   |  0.6387   |  0.03006  |  0.9617   |
|  43       |  0.6376   |  0.421    |  0.5779   |  0.9993   |
|  44       |  0.6189   |  2.027    |  0.9972   |  0.6877   |
|  45       |  0.6306   |  0.5667   |  0.3079   |  0.9951   |
|  46       |  0.6371   |  0.3283   |  0.8517   |  0.9986   |
|  47       |  0.6176   |  2.695    |  9.008e-0 |  1.0      |
|  48       |  0.6712   |  0.08295  |  0.106    |  0.9995   |  <<<<<<<<<<<< best player
|  49       |  0.5698   |  0.0      |  2.623e-0 |  1.0      |
|  50       |  0.5467   |  5.252    |  1.0      |  0.5501   |
|  51       |  0.1755   |  0.05044  |  0.9951   |  0.002986 |
|  52       |  0.5853   |  5.366    |  0.4848   |  0.0      |
|  53       |  0.6655   |  0.04123  |  0.3556   |  0.9807   |
|  54       |  0.5568   |  1.018    |  0.0      |  0.0      |
|  55       |  0.5901   |  3.367    |  0.9891   |  0.9678   |
|  56       |  0.5887   |  2.686    |  0.0      |  0.0      |
|  57       |  0.6084   |  2.908    |  1.0      |  0.4277   |
|  58       |  0.5904   |  4.176    |  0.3838   |  0.0      |
|  59       |  0.5855   |  5.44     |  0.006232 |  0.9934   |
|  60       |  0.5837   |  5.417    |  0.001906 |  0.02819  |
|  61       |  0.5531   |  4.773    |  1.0      |  1.0      |
|  62       |  0.5477   |  1.907    |  1.0      |  0.0      |
|  63       |  0.607    |  3.811    |  0.3691   |  0.9998   |
|  64       |  0.5905   |  4.714    |  0.5513   |  0.3975   |
|  65       |  0.5612   |  3.289    |  0.5772   |  0.008888 |
|  66       |  0.6069   |  1.454    |  1.0      |  0.4918   |
|  67       |  0.6107   |  4.256    |  0.0015   |  0.2458   |
|  68       |  0.6194   |  3.12     |  0.0      |  0.3942   |
|  69       |  0.6183   |  2.463    |  0.6086   |  0.5218   |
|  70       |  0.6178   |  2.029    |  0.6553   |  1.0      |
|  71       |  0.5825   |  4.242    |  1.0      |  0.4561   |
|  72       |  0.5917   |  4.344    |  0.5624   |  0.9978   |
|  73       |  0.5661   |  6.0      |  0.496    |  0.0      |
|  74       |  0.6482   |  0.1631   |  0.1918   |  0.8226   |
|  75       |  0.5823   |  0.6602   |  0.5181   |  0.3832   |


== Einleitung ==

==== Einleitung ====

* \pause Spiele als Lernumgebung für AI
** \pause 1951: Nimrod 
** \pause 1995: Vier gewinnt 
** \pause 1997: Schach 
** \pause 2016: Go

* \pause AlphaGo
* \pause AlphaZero
* \pause Großer Rechenaufwand nötig: 5000+ TPUs

==== Zielsetzung ====

* \pause Untersuche mögliche Effizienzsteigerungen
* \pause Solide Baseline
* \pause Experimentiere mit Vier gewinnt
* \pause Evaluiere verschiedende neue Ideen

== Grundlagen ==

=== Algorithmus ===

==== Monte Carlo tree search (MCTS) ====

* \pause Seit 2006 sehr verbreitet in Computer Go
* \pause Iterativer Baumsuchalgorithmus verwendet in AlphaGo/AlphaGoZero/AlphaZero
* \pause Benötigt:
** \pause Eine Policy die Züge einschätzen kann
** \pause Lösung um die Suchtiefe zu begrenzen
*** \pause Eine sehr schnelle Rolloutpolicy
*** \pause Alternativ: Policy zur Positionseinschätzung
* \pause Output: Eine bessere Policy über die Züge in der analysierten Position
* \pause MCTS ist praktisch ein Verbesserungsoperator

==== MCTS Beispiel ====

-<1>{\includegraphics[scale=0.225]{tree_1}}
-<2>{\includegraphics[scale=0.225]{tree_2}}
-<3>{\includegraphics[scale=0.225]{tree_3}}
-<4>{\includegraphics[scale=0.225]{tree_4}}
-<5>{\includegraphics[scale=0.225]{tree_5}}
-<6>{\includegraphics[scale=0.225]{tree_6}}
-<7>{\includegraphics[scale=0.225]{tree_7}}
-<8>{\includegraphics[scale=0.225]{tree_8}}


==== AlphaGo ====

* \pause Kernidee: Kombinere MCTS mit Deep Learning
* \pause Trainingsprozess involviert aber kein MCTS
* \pause Trainere mehrere Netzwerke
** \pause Supervised auf Datensatz von besten Menschen: Schnell und Langsam
** \pause Verbessere das langsame Netz durch RL
** \pause Erzeuge Datensatz für Netzwerk zur Positionsevaluierung
** \pause Verwende erzeugte Netzwerke um mit MCTS zu spielen
*** \pause Das langsame RL-Netzwerk macht die Ersteinschätzung der Züge
*** \pause Einschätzung der Spielposition: Netzwerk + Rollouts

==== AlphaZero ====

* \pause Drastische Vereinfachung von AlphaGo
* \pause Kernidee: Verwende MCTS bereits zur Trainingsphase
* \pause Verwendet nur ein Netzwerk: Positionsbewertung und Zugpolicy
* \pause Kein Bedarf für Datensatz von besten Menschen

==== AlphaZero: Trainingsablauf ====

<[center]
-<1>{\includegraphics[scale=0.425]{a0_cycle_0}}
-<2>{\includegraphics[scale=0.425]{a0_cycle_1}}
-<3>{\includegraphics[scale=0.425]{a0_cycle_2}}
-<4>{\includegraphics[scale=0.425]{a0_cycle_3}}
[center]>

=== Baselines ===

==== Extended baseline ====

* \pause Erweitere die Baselineimplementierung mit Verbesserungen anderer Arbeiten
* \pause Kombiniert ist die Verbesserung sehr erheblich

==== Extended baseline Ergebnisse ====

-<1>{\center \includegraphics[scale=0.625]{baseline_ex0}}
-<2>{\center \includegraphics[scale=0.625]{baseline_ex1}}
-<3>{\center \includegraphics[scale=0.625]{baseline_ex}}

== Untersuchte neue Ideen ==

=== Evolutionary Self-play ===

==== Implementierung: Evolutionary Self-play ====

* \pause Verwende die Selbstspielphase zur Evolution von Hyperparametern
* \pause Implementiert als eine Liga von Spielern
* \pause Ein Spieler ist ein Hyperparameterset
* \pause Bewerte Spieler mit Elo
* \pause Verwende Gaussian Mutation um die besten Spieler zu mutieren

* \pause Zwei Arten von Hyperparametern untersucht
** \pause Verwendung von Kullback-Leibler divergence um "Denkzeit" zu wählen.
** \pause MCTS Parameter: cpuct, fpu, drawValue

==== Erste Ergebnisse ====

-<1>{\center \includegraphics[scale=0.625]{evolve_results0}}
-<2>{\center \includegraphics[scale=0.625]{evolve_results1}}
-<3>{\center \includegraphics[scale=0.625]{evolve_results2}}
-<4>{\center \includegraphics[scale=0.625]{evolve_results}}

==== Untersuchung  ====

* \pause Bedingungen für erfolgreiche Evolution:
** \pause Die Liga muss gute Parameter erkennen
*** \pause Sie funktioniert
** \pause Viele Siege müssen sich übertragen auf schnelleren Lernfortschritt
*** \pause Dies ist das Problem
* \pause Viele gewonne Spiele bedeuten also nicht hoher Lernfortschritt
* \pause Nach hoher Siegesrate zu optimieren ist also nicht zielführend

=== Games as trees ===

==== Implementierung: Games as trees ====

* \pause Exploration durch Zurücksetzen an kritische Positionen
* \pause Spiele als MCTS-Baum
* \pause Notwendigkeit für MCTS-Evaluation Service
* \pause Nebeneffekt: Keine doppelte Auswertung von Positionen
-<6>{\center \includegraphics[scale=0.425]{cache_play0}}
-<7>{\center \includegraphics[scale=0.425]{cache_play1}}
-<8>{\center \includegraphics[scale=0.425]{cache_play}}


==== Zurücksetzen auf kritische Position ====

* \pause Nach einer Niederlage darf der Verlierer einen Zug zurücknehmen
* \pause Wähle Position anhand der Entwicklung der Positionsevaluation
* \pause Beginne neues Spiel in dieser Position
-<5>{\center \includegraphics[scale=0.425]{winp_tree0}}
-<6>{\center \includegraphics[scale=0.425]{winp_tree1}}
-<7>{\center \includegraphics[scale=0.425]{winp_tree}}

==== Spiele als MCTS-Baum ====

* \pause Exploration-Exploitation: MCTS macht das
* \pause Baue einen einzigen MCTS Baum
** \pause 150k+ Knoten
* \pause Reporte die Positionen in den Knoten als Trainingspositionen
-<6>{\center \includegraphics[scale=0.425]{mcts_tree_explore0}}
-<7>{\center \includegraphics[scale=0.425]{mcts_tree_explore1}}
-<8>{\center \includegraphics[scale=0.425]{mcts_tree_explore2}}
-<9>{\center \includegraphics[scale=0.425]{mcts_tree_explore3}}
-<10>{\center \includegraphics[scale=0.425]{mcts_tree_explore}}

=== Auxiliary features ===

==== Implementierung: Auxiliary features ====

* \pause Trainiere ein kleines Netzwerk, ca. 70k Parameter
* \pause Verwende interne Features aus diesem Netzwerk zur Regularisierung des großen Netzwerkes
* \pause Verschiedene Optionen wurden im Supervised Setting vorselektiert
* \pause Kleine Gewinne im Supervised Setting

==== Untersuchung ====
-<1>{\center \includegraphics[scale=0.425]{rndVsTrainedAux0}}
-<2>{\center \includegraphics[scale=0.425]{rndVsTrainedAux1}}
-<3>{\center \includegraphics[scale=0.425]{rndVsTrainedAux2}}
-<4>{\center \includegraphics[scale=0.425]{rndVsTrainedAux3}}
-<5>{\center \includegraphics[scale=0.425]{rndVsTrainedAux}}

==== Probleme ====

* \pause Zwei Probleme mit dem Ansatz:
** \pause Trainingskosten des kleinen Netzwerks
*** \pause Das Netzwerk mit dem Trainingslauf wachsen lassen hilft
** \pause Finale Spielstärke wird gestört
*** \pause Keine Lösung gefunden


== Ende ==

=== Fazit ===

==== Fazit ====

* \pause AlphaZero Experimentalframework entwickelt
* \pause Keine großen Verbesserungen gefunden
* \pause Trotzdem einiges interessante Erkenntnisse
** \pause Evolution für Hyperparameter funktioniert
*** \pause Nur eine gute Fitnessfunktion fehlt
*** \pause Unterschied zwischen Lernfortschritt und Spielstärke
** \pause Alternative Explorationsmethoden zeigen vor allem wie gut die einfache Standardversion funktioniert
** \pause Auxiliary Features aus dem inneren eines kleineren Netzwerks sind nur schwer nutzbar
* \pause Vorschläge für weitere Forschung
** \pause Suche nach Fitnessfunktion für Evolution
** \pause Das Konzept des Netzwerkwachstums sollte weiter erforscht werden

=== Referenzen ===

==== Referenzen ====

* Levente Kocsis et al.: Bandit based monte-carlo planning, 2006.
* David Silver et al.: Mastering the game of go with deep neural networks and tree search, 2016
* David Silver et al.: A general reinforcement learning algorithm that masters chess, shogi, and go through self-play, 2018

\centering Danke für Ihre Aufmerksamkeit.


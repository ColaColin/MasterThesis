% Dokumentenklasse fuer Artikel waehlen
\documentclass[12pt,onecolumn,oneside,titlepage]{article}

% Deutsche Grundeinstellungen und DIN A4
\usepackage{a4}

% Zur Einbindung von Grafiken mit \includegraphics
\usepackage[pdftex]{graphicx,color}
\DeclareGraphicsExtensions{.pdf,.jpg,.png,.eps,.ps}
\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage{textcomp,upquote,lmodern,listings}
\usepackage{subcaption}
\usepackage{float}

% Bibliographie mit BibTeX
%\usepackage{natbib}

% Mehrsprachige Bibliographie mit babelbib

\usepackage[english]{babel}
%\usepackage[ngerman]{babel}

\usepackage{babelbib}

% Korrekte Umsetzung von Umlauten
\usepackage[utf8]{inputenc}
\usepackage{times}

\usepackage{url}
\usepackage{hyperref}

% Mathematische Symbole
\usepackage[intlimits,centertags]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Einrueckung der ersten Zeile eines Absatzes
\setlength{\parindent}{0em}

% Abstand zwischen Absaetzen
\setlength{\parskip}{1.5ex plus0.5ex minus0.5ex}

% Seitenstil
% \pagestyle{headings}

% Seitennummerierung
\pagenumbering{arabic}
\setcounter{page}{2}

% Silbentrennungsliste
\hyphenation{native-Hello}

% -- Dokumentenbegin --------

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                              %%
%%                          Titelseite                          %%
%%                                                              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
{\huge \it Masterthesis}

\thispagestyle{empty}

\vspace{2cm}

{\Large \bf Investigation of possible improvements to increase the efficiency of the AlphaZero algorithm.}

\vspace{2.25cm}

\vspace{2.25cm}

{\large 
Christian-Albrechts-Universität zu Kiel \\
Institut für Informatik  \\
}

\end{center}

\vspace{2cm}

\begin{tabular}{ll}
angefertigt von:             & {\bf Colin Clausen} \\
betreuender Hochschullehrer: & Prof. Dr.-Ing. Sven Tomforde \\%
\end{tabular}

\vspace{1cm}

\begin{center}
Kiel, 20.7.2020
\end{center}


\pagebreak

\newpage\null\thispagestyle{empty}\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                              %%
%%                Selbstständigkeitserklärung                   %%
%%                                                              %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent {\bf Selbstständigkeitserklärung}

\vspace{1.5cm}

\noindent Ich erkläre hiermit, dass ich die vorliegende Arbeit selbstständig und nur unter Verwendung der angegebenen Literatur und Hilfsmittel angefertigt habe.

\vspace{2cm}
\noindent ............................................................... \\
Colin Clausen

\thispagestyle{empty}

\pagebreak

\newpage\null\thispagestyle{empty}\newpage


\tableofcontents

\pagebreak



\section{Introduction}

Games have been used for a long time as a standin of the more complex real world in developing artifical intelligence. Beating humans at various games has often been viewed as a milestone.

(TODO a few sentences here about progress on various milestone games).

In March 2016 a program called AlphaGo for the first time in history has defeated a top human player in the board game Go \cite{leesedolVsAlphaGo}.
Go had eluded attempts at super human level play for a very long time.

Louis Victor Allis attributes \cite{allis1994searching} this to the large game tree size of $10^{360}$ possible games, compared to $10^{120}$ in chess \cite{shannon1950xxii},
but also to the way humans use their natural pattern recognition ability to quickly eliminate most of the often 200 or more possible moves and focus on few promising ones.

This combination of the usage of hard-to-program pattern recognition with an extremely large game tree has prevented computers from reaching top human strength through game tree search algorithms based on programmed heuristics.
AlphaGo solved this issue by using the strong pattern recognition abilities of deep learning and combining them with a tree search, allowing the computer to learn patterns, similar to a human, but also search forward in the game tree to find the best move to play.

Further development of the AlphaGo algorithm yielded the AlphaZero algorithm, which significantly simplified AlphaGo, allowing learning to start with a random network and no requirements for human expert input of any kind.

In the following thesis I want to investigate further possible improvements to reduce the computational cost of using AlphaZero to learn to play games.

(TODO here one could state some key results, once they exist...)

This thesis is structured as follows: First I will look at previous work, starting with the basis of AlphaZero, Monte Carlo Tree Search, moving onto the various versions of AlphaZero with previously suggested improvements.
Then a list of novel improvements will be described. Finally an extensive set of experiments will be presented, first establishing a baseline performance, then showing the results on the novel improvements.


\section{Previous work}

In this section previous work relevant to AlphaZero will be presented.

\subsection{Monte Carlo Tree Search}

Monte Carlo Tree Search, in short MCTS, is the idea to search a large tree (e.g. a game tree) in a randomized fashion, gathering statistics on how good the expected return for moves at the root of the tree is.

An early suggestion of this idea was made by Bernd Brügmann in 1993 \cite{montecarlogo1993}, who suggested a tree search in a random fashion inspired by simulated annealing. He used the algorithm to play computer go and found promising results on 9x9 boards.

AlphaZero uses a variant of MCTS called UCT, which was formulated in 2006 \cite{kocsis2006bandit} and used in computer go for the first time in the same year \cite{gelly2006modification}.
UCT stands for ``UCB applied to trees'', where UCB is the ``Upper Confidence Bounds'' algorithm of the \emph{multi-armed bandit problem} \cite{auer2002finite}. 
Previous to AlphaZero multiple other strong computer go programs based on MCTS have been released, such as Pachi \cite{pachi_github} or CrazyStone \cite{crazystone}.


In the \emph{multi-armed bandit problem} a player is faced with a machine that has a set of levers, some of which return a high reward, while others return a low reward. The player is tasked with getting a high return from a fixed number of lever pulls.
This creates a dilemma, where the player has to decide to explore, i.e. try a new lever to maybe find a higher return, or exploit, i.e. pull the lever with the highest known reward. This dilemma is a key problem of reinforcement learning and L.Kocsis et al. \cite{kocsis2006bandit}
apply it to tree searches, effectively viewing the decision of which move to play in a node of the game tree as a multi-armed bandit problem.

UCT as described by L.Kocsis et al. is a rollout-based planning algorithm, which repeatedly samples possible episodes from the root of the tree. An episode is a possible sequence of moves that can be played form the root of the tree up to the end of the game. The result
of the episode is backpropagated upwards in the tree.
UCT is thus an incremental process, which improves the quality of the approximation of the move values at the root with every episode sampled and can be stopped at any time to return a result.

For every action $a$, state $s$, tree depth $d$ and time $t$ an implementation of UCT needs to track the estimated value of $a$ in $s$ at depth $d$ and time $t$ $Q_t(s,a,d)$, the number of visits of $s$ up to $d$ and $t$ $N_{s,d}(t)$ and the number of times
$a$ was selected in $s$ at depth $d$ and time $t$ $N_{s,a,d}(t)$.

A bias term, shown in equation \ref{eq:UCT_bias}, is defined where $C_p$ is a constant.

\begin{equation}
 C_{t,s} = 2C_p \sqrt{\frac{\ln t}{s}}\label{eq:UCT_bias}
\end{equation}


\begin{equation}
 Q_t(s,a,d) + C_{N_{s,d}(t), N_{s,a,d}(t)}\label{eq:UCT_max}
\end{equation}

MCTS with UCT selects actions at every node of the tree according to equation \ref{eq:UCT_max}, updating the visit counts and estimated values of nodes as episodes are completed.

Equation \ref{eq:UCT_max} can be understood as weighting exploitation, in the form of the $Q$ term, against exploration, in the form of the $C$ term. The specific form of $C_{t,s}$ is shown to be consistent and to have finite sample bounds on the estimation error by L.Kocsis et al.

\subsection{AlphaZero}

The AlphaZero algorithm \cite{silver2018general} is a simplification of AlphaGoZero \cite{silver2017mastering} and AlphaGo \cite{silver2016mastering}.
AlphaGo used a complicated system involving initialization with example games, random roleouts during tree searches and used multiple networks for different tasks. AlphaGoZero drastically simplified the system by only using a single network for all tasks,
and not doing any roleouts anymore, instead the network evaluation for the given positions is directly used.

The difference between AlphaGoZero and AlphaZero is mainly that AlphaGoZero involved comparing the currently trained network against the previously known best player by letting them play a set of evaluation games against each other.
Only the best player was used to generate new games. AlphaZero skips this and just always uses the current network to produce new games, surprisingly this appears to not give any disadvantage, learning remains stable.

The main advantage of the ``Zero`` versions is that they do not require any human knowledge about the game apart from the rules, the networks are trained from scratch by self-play alone.
This allows the algorithm to find the best way to play without human bias which seems to slightly increase final playing strength.
Additionally it allows to use the algorithm for research of games for which no human experts exist, such as No-Castling Chess \cite{NoCastleChess}.

\subsection{Extensions to AlphaZero}

Many improvements to the AlphaZero algorithm have been proposed, typically aiming at reducing the extreme computational cost of learning to play a new game.

\section{Evaluated novel improvements}
\subsection{Network modifications}
\subsection{Playing games as trees}
\subsection{Automatic auxilary features}

\section{Experiments}

\subsection{Baselines}
\subsubsection{AlphaZero implementation}
\subsubsection{Extended AlphaZero}


\subsection{Results on novel improvements}
\subsubsection{Network modifications}
\subsubsection{Playing games as trees}
\subsubsection{Automatic auxilary features}
\subsubsection{Evolutionary hyperparameters}






\pagebreak

\cite{silver2018general}

% -- Literaturverzeichnis --------

\bibliographystyle{plain}     % nummeriere Zitate [1], [2], ...

% Quellenangaben stehen in einer separaten BibTeX-Datei Seminararbeit.bib
\bibliography{document}

\end{document}
